<best_practices>
  <general_principles>
    <principle priority="high">
      <name>Test Before Release</name>
      <description>All code must be tested before being merged or deployed to production.</description>
      <rationale>Prevents broken features and regressions from reaching users.</rationale>
      <example>
        <scenario>Before merging a feature branch</scenario>
        <good>Run all automated and manual tests, report issues, block merge if failures found</good>
        <bad>Merge code without running tests or reviewing results</bad>
      </example>
    </principle>
    <principle priority="high">
      <name>Detailed Issue Reporting</name>
      <description>Every bug or failure must be reported with clear steps to reproduce, observed/expected behavior, and supporting evidence.</description>
      <rationale>Helps developers quickly understand and fix issues.</rationale>
      <example>
        <scenario>Test fails with error</scenario>
        <good>Include error logs, screenshots, and reproduction steps in the report</good>
        <bad>Report "it broke" with no details</bad>
      </example>
    </principle>
    <principle priority="medium">
      <name>Regression and Edge Case Testing</name>
      <description>Test not only new features but also existing functionality and edge cases.</description>
      <rationale>Ensures changes do not break unrelated parts of the system.</rationale>
      <example>
        <scenario>Adding a new database field</scenario>
        <good>Test old and new workflows, including empty/null values</good>
        <bad>Only test the new field, ignore existing features</bad>
      </example>
    </principle>
  </general_principles>

  <code_conventions>
    <convention category="test_naming">
      <rule>Use descriptive names for test functions and files</rule>
      <examples>
        <good>test_user_login_success.py, test_handle_empty_input()</good>
        <bad>test1.py, test_func()</bad>
      </examples>
    </convention>
    <convention category="structure">
      <rule>Organize tests by feature or module</rule>
      <template>
        tests/
          test_auth.py
          test_db_crud_async.py
          test_api_endpoints.py
      </template>
    </convention>
  </code_conventions>

  <common_pitfalls>
    <pitfall>
      <description>Ignoring failed tests or warnings</description>
      <why_problematic>Allows bugs to slip into production</why_problematic>
      <correct_approach>Investigate and resolve all failures before approving code</correct_approach>
    </pitfall>
    <pitfall>
      <description>Inadequate bug reports</description>
      <why_problematic>Slows down debugging and resolution</why_problematic>
      <correct_approach>Always provide full context and evidence in reports</correct_approach>
    </pitfall>
  </common_pitfalls>

  <quality_checklist>
    <category name="before_testing">
      <item>Review code changes and requirements</item>
      <item>Identify all affected features</item>
    </category>
    <category name="during_testing">
      <item>Run all relevant automated and manual tests</item>
      <item>Document failures with detail</item>
    </category>
    <category name="before_approval">
      <item>Verify all issues are resolved</item>
      <item>Confirm no regressions or new failures</item>
    </category>
  </quality_checklist>
</best_practices>